% ====================================================================
%+
% SECTION NAME:
%    lenstimedelays.tex
%
% CHAPTER:
%    cosmology.tex
%
% ELEVATOR PITCH:
%    Lensed quasars and supernovae provide distance measurements for
%    cosmology. They are a few days to a few weeks in length. To
%    measure them well we need long campaigns (>~3 years) with high
%    night-to-night cadence (better than the standard 5 days if
%    possible, especially as combining all filters might be difficult.)
%
% AUTHORS:
%   Phil Marshall (@drphilmarshall)
%-
% ====================================================================

\section{ Strong Gravitational Lens Time Delays }
\def\secname{lenstimedelays}\label{sec:\secname}

\credit{drphilmarshall},
\credit{rhiannonlynne},
\credit{tanguita}

The multiple images of strongly lensed quasars and supernovae have
delayed arrival times: variability in the first image will be observed
in the second image some time later, as the photons take different
paths around the deflector galaxy, and through different depths of
gravitational potential. If the lens mass distribution can be modeled
independently, using a combination of high resolution imaging of the
distorted quasar/SN host galaxy and stellar dynamics in the lens
galaxy, the measured time delays can be used to infer the ``time delay
distance'' in the system. This distance enables a direct measurement
of the Hubble constant, independent of the distance ladder.

% --------------------------------------------------------------------

\subsection{Target measurements and discoveries}
\label{sec:\secname:targets}

For this cosmological probe to be competitive with LSST's others, the
time delays of several hundred systems (which will be distributed
uniformly over the extragalactic sky) will need to be measured with
bias below the sub-percent level, while the precision required is a
few percent per lens.  In galaxy-scale lenses, the kind that are most
accurately modeled, these time delays are typically between several
days and several weeks long, and so are measurable in monitoring
campaigns having night-to-night cadence of between one and a few days,
and seasons lasting several months or more.

To obtain accurate as well as precise lensed quasar time delays, several
monitoring seasons are required. Lensed supernova time delays have not
yet been measured, but their transient nature means that their time
delay measurements may be more sensitive to cadence than season or
campaign length.

% --------------------------------------------------------------------

\subsection{Metrics}
\label{sec:\secname:metrics}

Anticipating that the time delay accuracy would depend on night-to-night
cadence, season length, and campaign length, we carried out a large
scale simulation and measurement program that coarsely sampled these
schedule properties. In \citet{LiaoEtal2015}, we simulated 5 different
light curve datasets, each containing 1000 lenses, and presented them to
the strong lensing community in a ``Time Delay Challenge.'' These 5
challenge ``rungs'' differed by their schedule properties, in the ways
shown in \autoref{tab:tdcrungs}. Focusing on the best challenge
submissions made by the community, we derived a simple power law model
for the variation of each of the time delay accuracy, time delay
precision, and useable sample fraction, with the schedule properties
cadence, season length and campaign length. These models are shown in
\autoref{fig:tdcresults}, reproduced from \citet{LiaoEtal2015}, and are
given by the following equations:
\begin{align}
|A|_{\rm model} &\approx 0.06\% \left(\frac{\rm cad} {\rm 3 days}  \right)^{0.0}
                          \left(\frac{\rm sea}  {\rm 4 months}\right)^{-1.0}
                          \left(\frac{\rm camp}{\rm 5 years} \right)^{-1.1} \notag \\
  P_{\rm model} &\approx 4.0\% \left(\frac{\rm cad} {\rm 3 days}  \right)^{ 0.7}
                         \left(\frac{\rm sea}  {\rm 4 months}\right)^{-0.3}
                         \left(\frac{\rm camp}{\rm 5 years} \right)^{-0.6} \notag \\
  f_{\rm model} &\approx 30\% \left(\frac{\rm cad} {\rm 3 days}  \right)^{-0.4}
                        \left(\frac{\rm sea}  {\rm 4 months}\right)^{ 0.8}
                        \left(\frac{\rm camp}{\rm 5 years} \right)^{-0.2} \notag
\end{align}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table*}
\begin{center}
\capstart
\begin{tabular}{cccccc} \hline\hline
  Rung &  Mean Cadence & Cadence Dispersion & Season   & Campaign & Length   \\
       &  (days)       & (days)             & (months) & (years)  & (epochs) \\ \hline
  0    &    3.0        &   1.0              &   8.0    &    5     & 400      \\
  1    &    3.0        &   1.0              &   4.0    &    10    & 400      \\
  2    &    3.0        &   0.0              &   4.0    &    5     & 200      \\
  3    &    3.0        &   1.0              &   4.0    &    5     & 200      \\
  4    &    6.0        &   1.0              &   4.0    &    10    & 200      \\
\hline\hline
\end{tabular}
\end{center}
\caption{The observing parameters for the five rungs of the Time Delay
Challenge. Reproduced from \citet{LiaoEtal2015}.\label{tab:tdcrungs}}
\end{table*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[!ht]
  \capstart
  \begin{minipage}[b]{\linewidth}
    \begin{minipage}[b]{0.32\linewidth}
      \centering\includegraphics[width=\linewidth]{figs/Accuracy_season_nca.pdf}
    \end{minipage} \hfill
    \begin{minipage}[b]{0.32\linewidth}
      \centering\includegraphics[width=\linewidth]{figs/Precision_cadence_nca.pdf}
    \end{minipage} \hfill
    \begin{minipage}[b]{0.32\linewidth}
      \centering\includegraphics[width=\linewidth]{figs/Fraction_season_nca.pdf}
    \end{minipage}
  \end{minipage}
\caption{Examples of changes in accuracy $A$ (left), precision $P$
(center) and success fraction $f$ (right) with schedule properties, as
seen in the different TDC submissions. The gray approximate power law
model was derived by visual inspection of the pyCS-SPL results; the
signs of the indices were pre-determined according to our expectations.
Reproduced from \citet{LiaoEtal2015}.}
\label{fig:tdcresults}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

All three of these metrics would, in an ideal world, be optimized:
this could be achieved by decreasing the night-to-night cadence (to
better sample the light curves), extending the observing season length
(to maximize the chances of capturing a strong variation and its
echo), and extending the campaign length (to increase the number of
effective time delay measurements). A combined figure of merit should
therefore be readily available.

The quantity of greatest scientific interest is the {\it accuracy in
cosmological parameters}: this coudl be computed as follows. Setting a
required accuracy threshold  defines the available number of lenses,
which in turns gives us the mean precision per lens there. Combining the
whole sample, we would get the error on the weighted mean time delay,
and can equate that to the statistical uncertainty on the Hubble
constant. The Figure of merit would be the final precision on $H_0$, as
a way to sum up the sample size and time delay measurability (at fixed
accuracy requirement).

% --------------------------------------------------------------------

\subsection{\OpSim Analysis}
\label{sec:\secname:analysis}

% \OpSim analysis: how good would the default observing strategy be, at
% the time of writing for this science project?

In this section we present the results of our ongoing \OpSim / MAF
analysis, as we try to
answer the question ``how good would the proposed observing
strategies be, for time delay lens cosmography?''

We used the
\simsMAFcontrib{SeasonStacker}{mafContrib/seasonStacker.py} to work
with seasons, rather than calendar years.
We used \texttt{ops2\_1075} \OpSim run for most of our tests, but plan
to re-run on \opsimdbref{db:baseCadence} and
\opsimdbref{db:NoVisitPairs}, in order to assess the new baseline
cadence and compare it against a simulated observing strategy where
the visit pair requirement is relaxed.

% \todo{PJM}{Correct the above paragraphs and add more links to MAF code.}

\autoref{fig:lenstimedelays:results} shows the results of our MAF
analysis of one \OpSim database, \texttt{ops2\_1075}, where we have
assumed that all filters were able to be used in the light curve
analysis (as was implicitly assumed when applying the results of
\citeauthor{LiaoEtal2015}). These sky maps show that, over the main
(WDF) survey area, the time delay accuracy, time delay precision and
time delay lens success fraction are consistently maintained,
indicating that the global average values of these metrics could
conceivably used as higher level metrics or even figure of merit.

% \autoref{tab:lenstimedelays:results} shows the global (i.e. all-sky)
% average values of our metrics, for two different \OpSim
% databases and two different filter set assumptions.
% \todo{PJM}{Compute global average lens time delay metrics and discuss.}
% \todo{PJM}{Define overall figure of merit and compute.}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[!ht]
  \capstart
  \begin{minipage}[b]{\linewidth}
    \begin{minipage}[b]{0.32\linewidth}
      \centering\includegraphics[width=\linewidth]{figs/lenstimedelays-ops2_1075-Accuracy-skymap.png}
    \end{minipage} \hfill
    \begin{minipage}[b]{0.32\linewidth}
      \centering\includegraphics[width=\linewidth]{figs/lenstimedelays-ops2_1075-Precision-skymap.png}
    \end{minipage} \hfill
    \begin{minipage}[b]{0.32\linewidth}
      \centering\includegraphics[width=\linewidth]{figs/lenstimedelays-ops2_1075-Fraction-skymap.png}
    \end{minipage}
  \end{minipage}
\caption{Sky maps of the accuracy $A$ (left), precision $P$ (center) and
success fraction $f$ (right) metrics, for the \texttt{ops2\_1075} \OpSim
database and assuming all filters ($ugrizy$) are used in the analysis
according to the assumptions described in the text.}
\label{fig:lenstimedelays:results}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table*}
\begin{center}
\caption{Lens Time Delay Metric Analysis Results.}
\label{tab:lenstimedelays:results}
\footnotesize
\begin{tabularx}{\linewidth}{ccccccccc}
  \hline
  \OpSim run
   & Filters
    & \texttt{cadence}
     & \texttt{season}
      & \texttt{campaign}
       & \texttt{Accuracy}
        & \texttt{Precision}
         & \texttt{Fraction}
          & \texttt{timedelayFoM} \\
  \hline\hline

  \opsimdbref{db:baseCadence}
   & $ri$
    & $XXX$
     & $XXX$
      & $XXX$
       & $XXX$
        & $XXX$
         & $XXX$
          & ??? \\

  \opsimdbref{db:baseCadence}
   & $ugrizy$
    & $XXX$
     & $XXX$
      & $XXX$
       & $XXX$
        & $XXX$
         & $XXX$
          & ??? \\
  \hline

  \opsimdbref{db:NoVisitPairs}
   & $ri$
    & $XXX$
     & $XXX$
      & $XXX$
       & $XXX$
        & $XXX$
         & $XXX$
          & ??? \\

  \opsimdbref{db:NoVisitPairs}
   & $ugrizy$
    & $XXX$
     & $XXX$
      & $XXX$
       & $XXX$
        & $XXX$
         & $XXX$
          & ??? \\
  \hline

\multicolumn{9}{p{\linewidth}}{\scriptsize Notes: see the text for
the definitions of each metric, and sky maps and histogram
plots of them. The Figure of Merit is still under development.}
\end{tabularx}
\normalsize
\medskip\\
\end{center}
\end{table*}
%%%%%%%%%%%%%%%%%%%%%%%%%%


% --------------------------------------------------------------------

\subsection{Discussion}
\label{sec:\secname:discussion}

% \todo{PJM}{Write lens time delays discussion section.}

The main risk involved with this science case
is that the multi-filter light curve analysis will not be well approximated by
the real-life combination of all 6 filters together.
The second time delay challenge (TDC2) will help answer this
question.  For now, just using 2 filters gives a lower limit on the
overall precision we should expect.

We would expect the relaxation of the visit pairs requirement to
increase the  night to night cadence by a factor of two, if the visits
are redistributed randomly in time. If \OpSim is not being as liberal as
this, we may not see much improvement over the baseline cadence:
efficiency maximization could be preventing visits being fully split. We
are interested in any changes to the WFD survey time sampling that
reduce the inter-night gaps: these  would include rolling cadence
schemes.

% Also need to assess 1 vs 3 vs 10 year light curves. How do metrics
% improve? What will be possible in the early part of the survey?

====================================================================

\subsection{Conclusions}

Based on the above results, we now answer the ten questions posed in
\autoref{sec:intro:evaluation:caseConclusions}:

\begin{description}

\item[Q1:] {\it Does the science case place any constraints on the
tradeoff between the sky coverage and coadded depth? For example, should
the sky coverage be maximized (to $\sim$30,000 deg$^2$, as e.g., in
Pan-STARRS) or the number of detected galaxies (the current baseline but
with 18,000 deg$^2$)?}

\item[A1:] Yes: not sure how this will play out though. Probs better to have smaller area and better data.

\item[Q2:] {\it Does the science case place any constraints on the
tradeoff between uniformity of sampling and frequency of sampling? For
example, a rolling cadence can provide enhanced sample rates over a part
of the survey or the entire survey for a designated time at the cost of
reduced sample rate the rest of the time (while maintaining the nominal
total visit counts).}

\item[A2:] Yes: higher frequency is better.

\item[Q3:] {\it Does the science case place any constraints on the
tradeoff between the single-visit depth and the number of visits
(especially in the $u$-band where longer exposures would minimize the
impact of the readout noise)?}

\item[A3:] Yes: more visits are better.

\item[Q4:] {\it Does the science case place any constraints on the
Galactic plane coverage (spatial coverage, temporal sampling, visits per
band)?}

\item[A4:] No. Only that time spent in the plane could be used to improve cadence, but effect is probably not large.

\item[Q5:] {\it Does the science case place any constraints on the
fraction of observing time allocated to each band?}

\item[A5:] No.

\item[Q6:] {\it Does the science case place any constraints on the
cadence for deep drilling fields?}

\item[A6:] To first order, no.

\item[Q7:] {\it Assuming two visits per night, would the science case
benefit if they are obtained in the same band or not?}

\item[A7:] Different bands are probably going to be better, but this has
not been tested.

\item[Q8:] {\it Will the case science benefit from a special cadence
prescription during commissioning or early in the survey, such as:
acquiring a full 10-year count of visits for a small area (either in all
the bands or in a  selected set); a greatly enhanced cadence for a small
area?}

\item[A8:] Not really. We rely on the difference imaging working well,
so making a good template across as much sky as possible would be good.
Including a known lens system (from DES, perhaps) in any deep field
would be useful too: we'd want the cadence to be similar to WDF to be
able to test our software.

\item[Q9:] {\it Does the science case place any constraints on the
sampling of observing conditions (e.g., seeing, dark sky, airmass),
possibly as a function of band, etc.?}

\item[A9:] No.

\item[Q10:] {\it Does the case have science drivers that would require
real-time exposure time optimization to obtain nearly constant
single-visit limiting depth?}

\item[A10:] No.

\end{description}

\navigationbar

% ====================================================================
